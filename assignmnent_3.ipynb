{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-N34gz3SVky"
      },
      "source": [
        "\n",
        "# **BERT BPE algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEKYNxykSe8x"
      },
      "outputs": [],
      "source": [
        "#import modules and packages\n",
        "import pip\n",
        "from importlib.util import find_spec\n",
        "\n",
        "required_packages = ['transformers', 'torch']\n",
        "\n",
        "for package in required_packages:\n",
        "  if find_spec(package) is None:\n",
        "    print(f'Installing package: {package}...')\n",
        "    pip.main(['install', package])\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForMaskedLM\n",
        "from transformers import AutoTokenizer\n",
        "from pprint import pprint\n",
        "\n",
        "#Defined variable for the pre-trained model and the tokenizer\n",
        "model_name = 'PlanTL-GOB-ES/roberta-base-bne'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name, output_hidden_states = True, )\n",
        "model.eval()\n",
        "#model_mask = pipeline('fill-mask', model=model_name)\n",
        "\n",
        "\n",
        "#In this variable, the list of words that will be analyzed is specified.\n",
        "word_list = [\"aceptable\", \"brevedad\", \"taxista\", \"consumismo\", \"poemario\", \"alumnado\", \"porrazo\", \"rosaleda\", \"portero\", \"zapatería\", \"preescolar\", \"irracional\", \"inaceptable\", \"subespecie\", \"antidisturbios\", \"sobrevolar\", \"contraataque\", \"deshacer\", \"protaurino\", \"monoparental\", \"mueble\", \"prepucio\", \"sorpresa\", \"oficina\", \"impresionar\", \"rústico\", \"técnica\", \"indicio\", \"rutina\", \"trigo\", \"verde\", \"ayer\", \"casa\", \"camisa\", \"álbum\", \"papel\", \"collar\", \"luna\", \"radio\", \"lejos\"]\n",
        "\n",
        "#Iterate over each word in the list\n",
        "for word in word_list:\n",
        "    marked_text = word\n",
        "    print(f'input: {marked_text}')\n",
        "\n",
        "    #Tokenize the sentence with the BERT tokenizer, which is based on the BPE algorithm.\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "    #Here, we print out the input, the tokenized word and the total number of tokens that the BPE algorithm has detected.\n",
        "    print(f'tokenized: {tokenized_text}')\n",
        "    print(f'number of tokens: {len(tokenized_text)}')\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcLBELdSLMb"
      },
      "source": [
        "# **OUR BPE tokenizer from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-eACt1JSQc9"
      },
      "outputs": [],
      "source": [
        "#Install and import libraries\n",
        "from collections import Counter, defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class BPE():\n",
        "    \"\"\"Byte-Pair Encoding: Subword-based tokenization algorithm.\"\"\"\n",
        "#Initialize BPE with corpus and number of iterations for merging\n",
        "    def __init__(self, corpus, num_iterations):\n",
        "        \"\"\"Initialize BPE tokenizer.\"\"\"\n",
        "        self.corpus = corpus\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "        # pre-tokenize the corpus into words, BERT pre-tokenizer is used here\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.word_freqs = defaultdict(int)\n",
        "        self.splits = {}\n",
        "        self.merges = {}\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train BPE tokenizer.\"\"\"\n",
        "\n",
        "        # compute the frequencies of each word in the corpus\n",
        "        for text in self.corpus:\n",
        "          #pre-tokenize the text using the BERT tokenizer\n",
        "            words_with_offsets = self.tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "           #extract words from pre-tokenization\n",
        "            new_words = [word for word, offset in words_with_offsets]\n",
        "            for word in new_words:\n",
        "                self.word_freqs[word] += 1\n",
        "\n",
        "        # compute the base vocabulary of all characters in the corpus\n",
        "        alphabet = []\n",
        "        for word in self.word_freqs.keys():\n",
        "            for letter in word:\n",
        "                if letter not in alphabet:\n",
        "                    alphabet.append(letter)\n",
        "        alphabet.sort()\n",
        "\n",
        "        # add the special token </w> at the beginning of the vocabulary\n",
        "        vocab = [\"</w>\"] + alphabet.copy()\n",
        "\n",
        "        # split each word into individual characters before training\n",
        "        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}\n",
        "\n",
        "        # merge the most frequent pair iteratively until the vocabulary size is reached\n",
        "        for i in range(self.num_iterations):\n",
        "\n",
        "            # compute the frequency of each pair\n",
        "            pair_freqs = self.compute_pair_freqs()\n",
        "\n",
        "            # find the most frequent pair\n",
        "            best_pair = \"\"\n",
        "            max_freq = None\n",
        "            for pair, freq in pair_freqs.items():\n",
        "                if max_freq is None or max_freq < freq:\n",
        "                    best_pair = pair\n",
        "                    max_freq = freq\n",
        "\n",
        "            # merge the most frequent pair\n",
        "            if i == self.num_iterations-1:\n",
        "              print('\\niteration', i)\n",
        "              print('vocabulary: ', vocab)\n",
        "              print('best pair:', best_pair)\n",
        "           #merge the best pair of subword units\n",
        "            self.splits = self.merge_pair(*best_pair)\n",
        "            self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
        "            vocab.append(best_pair[0] + best_pair[1])\n",
        "        return self.merges\n",
        "\n",
        "\n",
        "    def compute_pair_freqs(self):\n",
        "        \"\"\"Compute the frequency of each pair.\"\"\"\n",
        "        #initialize a dictionary to store the frequency of each pair\n",
        "        pair_freqs = defaultdict(int)\n",
        "        #iterate over each word and its frequency in the corpus\n",
        "        for word, freq in self.word_freqs.items():\n",
        "            split = self.splits[word]  #and split subword units for the current word\n",
        "           #if the word consists of 1 subword unit, skip to the next one\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "         #iterate over each pair of subword units in the split above\n",
        "            for i in range(len(split) - 1):\n",
        "                pair = (split[i], split[i + 1])\n",
        "                pair_freqs[pair] += freq\n",
        "        return pair_freqs\n",
        "\n",
        "\n",
        "    def merge_pair(self, a, b):\n",
        "        \"\"\"Merge the given pair.\"\"\"\n",
        "        #iterate over each word\n",
        "        for word in self.word_freqs:\n",
        "            split = self.splits[word]\n",
        "          #if the word consists of 1 subword units, skip to the next one\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "            i = 0\n",
        "            #If the current pair of subword units matches the given pair (a, b),\n",
        "            #merge them by replacing them with the concatenated subword unit (a + b)\n",
        "            while i < len(split) - 1:\n",
        "                if split[i] == a and split[i + 1] == b:\n",
        "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
        "                #If the current pair does not match, move to the next pair\n",
        "                else:\n",
        "                    i += 1\n",
        "            self.splits[word] = split\n",
        "        return self.splits\n",
        "\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize a given text with trained BPE tokenizer (including pre-tokenization, split, and merge).\"\"\"\n",
        "\n",
        "        pre_tokenize_result = self.tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "        pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
        "        #Split the pre-tokenized text into individual characters\n",
        "        splits_text = [[l for l in word] for word in pre_tokenized_text]\n",
        "        #Iterate over each pair and its corresponding merge operation\n",
        "        for pair, merge in self.merges.items():\n",
        "          #iterate over each spllitted text\n",
        "            for idx, split in enumerate(splits_text):\n",
        "                i = 0\n",
        "              #If the current pair of characters matches the pair to merge,\n",
        "              #replace them with the merged subword unit\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [merge] + split[i + 2 :]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits_text[idx] = split\n",
        "        result = sum(splits_text, [])\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG4TN8JASkqp",
        "outputId": "49a47013-b6b7-4e26-ecfe-e5f0767c36f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffThe Project Gutenberg eBook of Niebla (Nivola)\\n', '    \\n', 'This ebook is for the use of anyone anywhere in the United States and\\n', 'most other parts of the world at no cost and with almost no restrictions\\n', 'whatsoever. You may copy it, give it away or re-use it under the terms\\n']\n",
            "\n",
            "iteration 9\n",
            "vocabulary:  ['</w>', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '«', '»', '¿', 'Á', 'É', 'Í', 'Ó', 'Ú', 'á', 'è', 'é', 'í', 'ñ', 'ó', 'ú', 'ü', '—', '‘', '’', '“', '”', '•', '™', '\\ufeff', 'en', 'es', 'er', 'qu', 'de', 'la', 'os', 'que', 'ar']\n",
            "best pair: ('d', 'o')\n",
            "{('e', 'n'): 'en', ('e', 's'): 'es', ('e', 'r'): 'er', ('q', 'u'): 'qu', ('d', 'e'): 'de', ('l', 'a'): 'la', ('o', 's'): 'os', ('qu', 'e'): 'que', ('a', 'r'): 'ar', ('d', 'o'): 'do'}\n",
            "['a', 'c', 'e', 'p', 't', 'a', 'b', 'l', 'e']\n",
            "['b', 'r', 'e', 'v', 'e', 'd', 'a', 'd']\n",
            "['t', 'a', 'x', 'i', 's', 't', 'a']\n",
            "['c', 'o', 'n', 's', 'u', 'm', 'i', 's', 'm', 'o']\n",
            "['p', 'o', 'e', 'm', 'ar', 'i', 'o']\n",
            "['a', 'l', 'u', 'm', 'n', 'a', 'do']\n",
            "['p', 'o', 'r', 'r', 'a', 'z', 'o']\n",
            "['r', 'os', 'a', 'l', 'e', 'd', 'a']\n",
            "['p', 'o', 'r', 't', 'er', 'o']\n",
            "['z', 'a', 'p', 'a', 't', 'er', 'í', 'a']\n",
            "['p', 'r', 'e', 'es', 'c', 'o', 'la', 'r']\n",
            "['i', 'r', 'r', 'a', 'c', 'i', 'o', 'n', 'a', 'l']\n",
            "['i', 'n', 'a', 'c', 'e', 'p', 't', 'a', 'b', 'l', 'e']\n",
            "['s', 'u', 'b', 'es', 'p', 'e', 'c', 'i', 'e']\n",
            "['a', 'n', 't', 'i', 'd', 'i', 's', 't', 'u', 'r', 'b', 'i', 'os']\n",
            "['s', 'o', 'b', 'r', 'e', 'v', 'o', 'la', 'r']\n",
            "['c', 'o', 'n', 't', 'r', 'a', 'a', 't', 'a', 'que']\n",
            "['d', 'es', 'h', 'a', 'c', 'er']\n",
            "['p', 'r', 'o', 't', 'a', 'u', 'r', 'i', 'n', 'o']\n",
            "['m', 'o', 'n', 'o', 'p', 'ar', 'en', 't', 'a', 'l']\n",
            "['m', 'u', 'e', 'b', 'l', 'e']\n",
            "['p', 'r', 'e', 'p', 'u', 'c', 'i', 'o']\n",
            "['s', 'o', 'r', 'p', 'r', 'es', 'a']\n",
            "['o', 'f', 'i', 'c', 'i', 'n', 'a']\n",
            "['i', 'm', 'p', 'r', 'es', 'i', 'o', 'n', 'ar']\n",
            "['r', 'ú', 's', 't', 'i', 'c', 'o']\n",
            "['t', 'é', 'c', 'n', 'i', 'c', 'a']\n",
            "['i', 'n', 'd', 'i', 'c', 'i', 'o']\n",
            "['r', 'u', 't', 'i', 'n', 'a']\n",
            "['t', 'r', 'i', 'g', 'o']\n",
            "['v', 'er', 'de']\n",
            "['a', 'y', 'er']\n",
            "['c', 'a', 's', 'a']\n",
            "['c', 'a', 'm', 'i', 's', 'a']\n",
            "['á', 'l', 'b', 'u', 'm']\n",
            "['p', 'a', 'p', 'e', 'l']\n",
            "['c', 'o', 'l', 'la', 'r']\n",
            "['l', 'u', 'n', 'a']\n",
            "['r', 'a', 'd', 'i', 'o']\n",
            "['l', 'e', 'j', 'os']\n"
          ]
        }
      ],
      "source": [
        "#import the Wikipedia corpus used for training\n",
        "with open('corpus.txt', encoding=\"utf8\") as f:\n",
        "    corpus = f.readlines()\n",
        "    print(corpus[:5])\n",
        "\n",
        "# create a BPE tokenizer object\n",
        "MyBPE = BPE(corpus=corpus, num_iterations=10000) #here we can adjust the number of iterations\n",
        "\n",
        "# train BPE tokenizer with Wikipedia corpus\n",
        "print(MyBPE.train())\n",
        "\n",
        "# tokenize the given text\n",
        "word_list =[\"aceptable\", \"brevedad\", \"taxista\", \"consumismo\", \"poemario\", \"alumnado\", \"porrazo\", \"rosaleda\", \"portero\", \"zapatería\", \"preescolar\", \"irracional\", \"inaceptable\", \"subespecie\", \"antidisturbios\", \"sobrevolar\", \"contraataque\", \"deshacer\", \"protaurino\", \"monoparental\", \"mueble\", \"prepucio\", \"sorpresa\", \"oficina\", \"impresionar\", \"rústico\", \"técnica\", \"indicio\", \"rutina\", \"trigo\", \"verde\", \"ayer\", \"casa\", \"camisa\", \"álbum\", \"papel\", \"collar\", \"luna\", \"radio\", \"lejos\"]\n",
        "for word in word_list:\n",
        "    marked_text = word\n",
        "    print(MyBPE.tokenize(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMApYn1rRyot"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}